name: local-llm-rag
services:
  # ---------------- LLM runtime (Ollama) ----------------
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"                        # Ollama API
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS=/models
    volumes:
      - ollama-models:/models
    gpus: all                                 # requires nvidia-container-toolkit
    healthcheck:
      # Checks BOTH: 1) Ollama API is up  2) GPU visible via nvidia-smi
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:11434/api/tags >/dev/null && command -v nvidia-smi >/dev/null 2>&1 && nvidia-smi -L"]
      interval: 10s
      timeout: 5s
      retries: 30

  # ---- One-shot seeding: auto-pull your default models on first boot ----
  seed-models:
    image: docker.io/curlimages/curl:8.10.1
    container_name: seed-models
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      # Change these if you want different defaults
      - DEFAULT_CHAT_MODEL=llama3.1:8b
      - DEFAULT_EMBED_MODEL=nomic-embed-text
    entrypoint: ["/bin/sh","-lc"]
    command: >
      'set -e;
       for m in "$$DEFAULT_CHAT_MODEL" "$$DEFAULT_EMBED_MODEL"; do
         if [ -n "$$m" ]; then
           echo "Pulling $$m ..."
           curl -fsS -X POST http://ollama:11434/api/pull -d "{\"name\":\"$$m\"}";
         fi
       done;
       echo "Model seeding complete."'
    restart: "no"

  # ---------------- Web UI ----------------
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "3000:8080"                           # Web UI at http://localhost:3000
    volumes:
      - openwebui-data:/app/backend/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8080 >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30

  # ---------------- Vector DB (Qdrant) ----------------
  qdrant:
    image: docker.io/qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"                           # REST
      - "6334:6334"                           # gRPC
    volumes:
      - qdrant-data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:6333/readyz >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30

volumes:
  ollama-models:
  openwebui-data:
  qdrant-data:

